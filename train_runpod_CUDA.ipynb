{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9419fcd-a96c-47e8-a16c-8b738f775dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load data\n",
    "with open(\"processed_sequences.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(\"chord_mappings.pkl\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "# input/target/genre sequences\n",
    "input_sequences = data[\"input_sequences\"]\n",
    "target_sequences = data[\"target_sequences\"]\n",
    "genres_for_sequences = data[\"genres_for_sequences\"]\n",
    "\n",
    "# chord_mappings\n",
    "chord_to_id = mappings[\"chord_to_id\"]\n",
    "id_to_chord = mappings[\"id_to_chord\"]\n",
    "\n",
    "# genre_mappings\n",
    "genres = sorted(set(genres_for_sequences))\n",
    "genre_to_id = {genre: i for i, genre in enumerate(genres)}\n",
    "id_to_genre = {i: genre for genre, i in genre_to_id.items()}\n",
    "\n",
    "# Encode genre labels\n",
    "encoded_genres = [genre_to_id[g] for g in genres_for_sequences]\n",
    "\n",
    "# tokenizer\n",
    "def tokenize_chords(chord_seq):\n",
    "    return [chord_to_id.get(chord, chord_to_id[\"UNK\"]) for chord in chord_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "MAX_SEQ_LEN = 200  # Set the maz sequence length to 200\n",
    "\n",
    "class ChordPredictionDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, genres, chord_to_id, genre_to_id):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.genres = [genre_to_id[g] for g in genres]\n",
    "        self.pad_id = chord_to_id[\"PAD\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx],\n",
    "            \"target_ids\": self.targets[idx],\n",
    "            \"genre_id\": self.genres[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a12d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_seqs = [item[\"input_ids\"] for item in batch]\n",
    "    target_seqs = [item[\"target_ids\"] for item in batch]\n",
    "    genre_ids = [item[\"genre_id\"] for item in batch]\n",
    "\n",
    "    # Pad or truncate sequences to MAX_SEQ_LEN\n",
    "    pad = lambda seqs: [\n",
    "        (seq[:MAX_SEQ_LEN] + [chord_to_id[\"PAD\"]] * (MAX_SEQ_LEN - len(seq))) if len(seq) < MAX_SEQ_LEN else seq[:MAX_SEQ_LEN]\n",
    "        for seq in seqs\n",
    "    ]\n",
    "    mask = lambda seqs: [\n",
    "        [1]*len(seq) + [0]*(MAX_SEQ_LEN - len(seq)) if len(seq) < MAX_SEQ_LEN else [1]*MAX_SEQ_LEN\n",
    "        for seq in seqs\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(pad(input_seqs), dtype=torch.long),\n",
    "        \"target_ids\": torch.tensor(pad(target_seqs), dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(mask(input_seqs), dtype=torch.long),\n",
    "        \"genre_id\": torch.tensor(genre_ids, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "dataset = ChordPredictionDataset(input_sequences, target_sequences, genres_for_sequences, chord_to_id, genre_to_id)\n",
    "\n",
    "# train with 10000 samples \n",
    "small_dataset = Subset(dataset, list(range(min(10000, len(dataset)))))\n",
    "\n",
    "dataloader = DataLoader(small_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from genre_aware_model import build_transformer\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "print(device)\n",
    "model = build_transformer(\n",
    "    src_vocab_size=len(chord_to_id),\n",
    "    tgt_vocab_size=len(chord_to_id),\n",
    "    src_seq_len=200,\n",
    "    tgt_seq_len=200,\n",
    "    genre_len=len(genre_to_id),\n",
    "    d_model=256,\n",
    "    ffn_size=256, \n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.NLLLoss(ignore_index=chord_to_id[\"PAD\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # each batch: input_ids, target_ids, genre_id\n",
    "    # input_ids: [batch_size, seq_len]\n",
    "    # target_ids: [batch_size, seq_len]\n",
    "    # genre_id: [batch_size]\n",
    "    # attention_mask: [batch_size, seq_len]\n",
    "    # tgt_input: [batch_size, seq_len]\n",
    "    # tgt_output: [batch_size, seq_len]\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}\", ncols=100, leave=False)):\n",
    "\n",
    "        src = batch[\"input_ids\"].to(device)\n",
    "        tgt_input = batch[\"input_ids\"].to(device)\n",
    "        tgt_output = batch[\"target_ids\"].to(device)\n",
    "        genre = batch[\"genre_id\"].to(device)\n",
    "        src_mask = batch[\"attention_mask\"].unsqueeze(1).unsqueeze(2).to(device)\n",
    "        tgt_mask = torch.tril(torch.ones((src.size(1), src.size(1)))).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        enc_out = model.encode(src, genre, src_mask=src_mask)\n",
    "        dec_out = model.decode(tgt_input, enc_out, genre, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        logits = model.project(dec_out)\n",
    "\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), tgt_output.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect() \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        tqdm.write(f\"Batch {i+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Total Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"transformer_chord_predictor_epoch{epoch+1}.pt\")\n",
    "\n",
    "torch.save(model.state_dict(), \"transformer_chord_predictor_final.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
