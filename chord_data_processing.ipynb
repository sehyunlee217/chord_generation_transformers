{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4b9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seunghyunlee/Desktop/transformer_chord_generation/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/qm/5ggtfrm15ql9cqr3cfjzvy4r0000gn/T/ipykernel_63398/1972042967.py:5: DtypeWarning: Columns (2,3,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"hf://datasets/ailsntua/Chordonomicon/chordonomicon_v2.csv\")\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/04 23:17:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/04 23:17:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/04 23:17:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/04 23:17:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/04 23:17:50 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/04 23:17:50 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/04 23:17:50 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/04 23:17:50 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/04 23:18:05 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as spark_F\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/ailsntua/Chordonomicon/chordonomicon_v2.csv\")\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder # builder pattern abstraction \n",
    "        .appName(\"Chord Progression Prediction\")\n",
    "        .config(\"spark.driver.memory\", \"16g\")\n",
    "        .getOrCreate() # work both in batch & interactive mode \n",
    "        )\n",
    "\n",
    "chords_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d265d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chords = chords_df.select(\n",
    "    spark_F.col(\"id\"), \n",
    "    spark_F.col(\"main_genre\").alias(\"genre\"),\n",
    "    spark_F.split(spark_F.col(\"chords\"), \" \").alias(\"chord_arrays\"))\\\n",
    "        .select(spark_F.col(\"id\"), spark_F.col(\"genre\"), spark_F.col(\"chord_arrays\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d4d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, StringType, ArrayType\n",
    "\n",
    "@spark_F.udf(returnType=MapType(StringType(), ArrayType(StringType())))\n",
    "def split_progression_by_sections(chord_array):\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    \n",
    "    for item in chord_array:\n",
    "        # if item is a section marker\n",
    "        if item.startswith('<') and item.endswith('>'):\n",
    "            current_section = item[1:-1]\n",
    "            sections[current_section] = []\n",
    "        elif current_section is not None:\n",
    "            sections[current_section].append(item)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "result_df = flat_chords.withColumn(\n",
    "    \"sections\", \n",
    "    split_progression_by_sections(\"chord_arrays\")\n",
    ")\n",
    "\n",
    "sections_df = result_df.select(\n",
    "    \"genre\",\n",
    "    spark_F.explode(\"sections\").alias(\"section_name\", \"chords\")\n",
    ")\n",
    "\n",
    "genre_chords_df = sections_df.select(spark_F.col(\"genre\"), spark_F.col(\"chords\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737f2deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 23:18:47 WARN TaskSetManager: Stage 0 contains a task of very large size (38752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/04 23:18:52 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------+\n",
      "|genre|chords                                                       |\n",
      "+-----+-------------------------------------------------------------+\n",
      "|pop  |[F, C, E7, Amin, C, F, C, G7, C, F, C, E7, Amin, C, F, G7, C]|\n",
      "|pop  |[G, D, G, D, A, D, G, D, Fs7, Bmin, D, G, A7, D, G, A7, D]   |\n",
      "|pop  |[F, C, F, C, G, C, F, C, E7, Amin, C, F, G7, C]              |\n",
      "|pop  |[C]                                                          |\n",
      "|pop  |[D]                                                          |\n",
      "+-----+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre_chords_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6aa8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 23:19:07 WARN TaskSetManager: Stage 1 contains a task of very large size (38752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/04 23:19:24 WARN TaskSetManager: Stage 2 contains a task of very large size (38752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/04 23:19:42 WARN TaskSetManager: Stage 3 contains a task of very large size (38752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/04 23:20:07 WARN TaskSetManager: Stage 6 contains a task of very large size (38752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "genres_list = sections_df.select(spark_F.col(\"genre\")).distinct()\n",
    "\n",
    "genre_chords_df = genre_chords_df.where(spark_F.col(\"genre\") != \"NaN\")\\\n",
    "    .where(spark_F.size(spark_F.col(\"chords\")) >= 5) # only keep chord array with 5 or more chords\n",
    "\n",
    "sequences = genre_chords_df.select(\"chords\").rdd.map(lambda r: r[0]).collect()\n",
    "genre_for_sequences = genre_chords_df.select(\"genre\").rdd.map(lambda r: (r[0])).collect()\n",
    "\n",
    "# Distinct genres\n",
    "genres = genres_list.select(\"genre\").rdd.map(lambda r: r[0]).collect()\n",
    "# Distinct chords\n",
    "distinct_chords = genre_chords_df.select(spark_F.explode(\"chords\")).distinct().rdd.map(lambda r: r[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fde9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab list\n",
    "special_tokens = [\"PAD\", \"UNK\"]\n",
    "chord_vocab = special_tokens + distinct_chords\n",
    "chord_to_id = {chord: i for i, chord in enumerate(chord_vocab)}\n",
    "id_to_chord = {i: chord for chord, i in chord_to_id.items()}\n",
    "\n",
    "# tokenizer\n",
    "def tokenize_chords(chord_seq):\n",
    "    return [chord_to_id.get(chord, chord_to_id[\"UNK\"]) for chord in chord_seq]\n",
    "\n",
    "# chord -> token\n",
    "tokenized_sequences = [tokenize_chords(seq) for seq in sequences]\n",
    "\n",
    "# Genres to IDs\n",
    "genre_to_id = {genre: i for i, genre in enumerate(genres)}\n",
    "id_to_genre = {i: genre for genre, i in genre_to_id.items()}\n",
    "\n",
    "encoded_genres = [genre_to_id[g] for g in genre_for_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178ed921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving chord_to_id and id_to_chord dictionaries\n",
    "with open(\"chord_mappings.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"chord_to_id\": chord_to_id, \"id_to_chord\": id_to_chord}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift 1 to left for input, and right 1 for target\n",
    "input_sequences = [seq[:-1] for seq in tokenized_sequences if len(seq) > 1]\n",
    "target_sequences = [seq[1:] for seq in tokenized_sequences if len(seq) > 1]\n",
    "genres_for_sequences = [genre for seq, genre in zip(tokenized_sequences, genre_for_sequences) if len(seq) > 1]\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_to_save = {\n",
    "    \"input_sequences\": input_sequences,\n",
    "    \"target_sequences\": target_sequences,\n",
    "    \"genres_for_sequences\": genres_for_sequences\n",
    "}\n",
    "\n",
    "with open(\"processed_sequences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1f5c5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
